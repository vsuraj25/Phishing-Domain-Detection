{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from urllib.parse import urlparse\n",
    "import ssl\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.gsb_api =  'AIzaSyBMgu902O7m88pyKQCcheEFN3ZNWhib0oM'\n",
    "        self.vt_api = '3a45ada00deb8c44a97a7d565752f4f10c767081595e44a1a2d85eddc1628254'\n",
    "        self.us_api = '73fa8163-46a4-458e-a655-bf97e628ffa7'\n",
    "        self.whois_api = 'at_ULzhOZ4s92D1qgjtjxqEmNuhJNiU6'\n",
    "        self.opr_api = 'scggscscw84soko0c8g4440gso0o8sgkkggkkc88'\n",
    "        trial_domain = self._get_domain()\n",
    "        print(trial_domain)\n",
    "\n",
    "    def _get_domain(self):\n",
    "        try:\n",
    "            domain = urlparse(self.url).netloc\n",
    "            if domain == \"\":\n",
    "                domain_regex = r'^(?:https?:\\/\\/)?(?:www\\.)?([a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+)'\n",
    "                match = re.match(domain_regex, self.url)\n",
    "                if match:\n",
    "                    domain = match.group(1)\n",
    "            return domain\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self._get_domain.__name__} failed returning default\" )\n",
    "            raise e\n",
    "        \n",
    "    def _get_domain_external(self, url):\n",
    "        try:\n",
    "            domain = urlparse(url).netloc\n",
    "            if domain == \"\":\n",
    "                domain_regex = r'^(?:https?:\\/\\/)?(?:www\\.)?([a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+)'\n",
    "                match = re.match(domain_regex, url)\n",
    "                if match:\n",
    "                    domain = match.group(1)\n",
    "            return domain\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self._get_domain.__name__} failed returning default\" )\n",
    "            raise e\n",
    "        \n",
    "    def _extract_links_from_strings(self,string_list):\n",
    "        all_links = []\n",
    "\n",
    "        for string in string_list:\n",
    "            # Find all URLs in the string using regular expressions\n",
    "            urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', string)\n",
    "\n",
    "            # If no URLs found, check for URLs without prefix and add the prefix\n",
    "            if not urls:\n",
    "                urls_without_prefix = re.findall(r'(?<!\\w)([-\\w.]+\\.[a-zA-Z]{2,6}(?:\\/\\S*)?)', string)\n",
    "                urls = ['http://' + url for url in urls_without_prefix]\n",
    "\n",
    "            all_links.extend(urls)\n",
    "\n",
    "        return all_links\n",
    "    \n",
    "    def _get_dissimilarity(self,parsed_links, domain):\n",
    "        if len(parsed_links) > 0:\n",
    "            domain_disimilarity = []\n",
    "            for link in parsed_links:\n",
    "                if self._get_domain_external(link) == domain:\n",
    "                    domain_disimilarity.append(0)\n",
    "                else: \n",
    "                    domain_disimilarity.append(1)\n",
    "            similarity_perc = (sum(domain_disimilarity) * len(domain_disimilarity)) / 100\n",
    "            return similarity_perc\n",
    "\n",
    "        else:\n",
    "            print('No parsed links found')\n",
    "            print(f\"Function{self._get_dissimilarity.__name__} failed returning default\" )\n",
    "            return 0\n",
    "        \n",
    "    def _check_form(self,url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "\n",
    "            if 'text/html' in content_type and 'form' in response.text.lower():\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Function{self._check_form.__name__} failed returning default\" )\n",
    "            return False\n",
    "        \n",
    "    def _get_response_with_https(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "    \n",
    "        if not parsed_url.scheme:\n",
    "            url = \"https://\" + url\n",
    "            \n",
    "        response = requests.get(url, timeout= 4)\n",
    "        return response\n",
    "    \n",
    "    def _extract_hostname(self, url):\n",
    "        pattern = r\"(?:(?:http|https|ftp):\\/\\/)?(?:www\\.)?([^\\/]+)\"\n",
    "        match = re.match(pattern, url)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return self._get_domain(url)\n",
    "        \n",
    "    def _get_response(self,url):\n",
    "        return requests.get(url, timeout=3)\n",
    "    \n",
    "    def _get_whois_data(self):\n",
    "        domain = self._get_domain()\n",
    "        # API endpoint and parameters\n",
    "        endpoint = 'https://www.whoisxmlapi.com/whoisserver/WhoisService'\n",
    "        params = {\n",
    "            'apiKey': self.whois_api,\n",
    "            'domainName': domain,\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _http_request(self,url):\n",
    "        try:\n",
    "            parsed_url = urlparse(url)\n",
    "            if not parsed_url.scheme:\n",
    "                url = \"https://\" + self.url\n",
    "                return url\n",
    "            else:\n",
    "                return url\n",
    "        except:\n",
    "            print(f\"Function{self._http_request.__name__} failed returning default\" )\n",
    "            return url\n",
    "                    \n",
    "    \n",
    "    ## --------------- Features ------------------\n",
    "\n",
    "    ## 1 - having_IP_Address\n",
    "    def having_IP_Address(self):\n",
    "        try:\n",
    "            # Regular expression to match an IP address\n",
    "            ip_regex = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
    "\n",
    "            # Find all IP addresses in the URL\n",
    "            ip_addresses = re.findall(ip_regex, self.url)\n",
    "\n",
    "            # If at least one IP address is found, return True\n",
    "            if len(ip_addresses) > 0:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            print(f\"Function{self.having_IP_Address.__name__} failed returning default\" )\n",
    "            return 1\n",
    "    \n",
    "    ## 2 - URL_Length\n",
    "    def URL_Length(self):\n",
    "        if len(self.url) < 54:\n",
    "            return 1\n",
    "        if len(self.url) >= 54 and len(self.url) <= 75:\n",
    "            return 0\n",
    "        return -1\n",
    "    \n",
    "    ## 3 - Shortining_Service\n",
    "    def Shortining_Service(self):\n",
    "        match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net', self.url)\n",
    "        if match:\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    ## 4 - having_At_Symbol\n",
    "    def having_At_Symbol(self):\n",
    "        if re.findall(\"@\",self.url):\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 5 - double_slash_redirecting\n",
    "    def double_slash_redirecting(self):\n",
    "        if self.url.rfind('//')>6:\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    ## 6 - Prefix_Suffix\n",
    "    def Prefix_Suffix(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            match = re.findall('\\-', domain)\n",
    "            if match:\n",
    "                return -1\n",
    "            return 1\n",
    "        except:\n",
    "            print(f\"Function{self.Prefix_Suffix.__name__} failed returning default\" )\n",
    "            return 1\n",
    "        \n",
    "    ## 7 - having_Sub_Domain\n",
    "    def having_Sub_Domain(self): \n",
    "        dot_count = len(re.findall(\"\\.\", self.url)) - 1\n",
    "        if 'www.' in self.url:\n",
    "             dot_count -=  1\n",
    "        if dot_count == 1:\n",
    "            return 1\n",
    "        elif dot_count == 2:\n",
    "            return 0\n",
    "        return -1\n",
    "    \n",
    "    ## 8 - SSLfinal_State\n",
    "    def SSLfinal_State(self):\n",
    "\n",
    "        def using_https(url):\n",
    "            if url.startswith('https'):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "        def trust_check_googlesafebrowsing(url, api_key):\n",
    "            api_url = 'https://safebrowsing.googleapis.com/v4/threatMatches:find'\n",
    "\n",
    "            payload = {\n",
    "                \"client\": {\n",
    "                    \"clientId\": \"795201079937-90cocmnhrjk9l2717tv4lcaip4rg83j2.apps.googleusercontent.com\",\n",
    "                    \"clientVersion\": \"1.0\"\n",
    "                },\n",
    "                \"threatInfo\": {\n",
    "                    \"threatTypes\": [\"MALWARE\", \"SOCIAL_ENGINEERING\", \"POTENTIALLY_HARMFUL_APPLICATION\"],\n",
    "                    \"platformTypes\": [\"ANY_PLATFORM\"],\n",
    "                    \"threatEntryTypes\": [\"URL\"],\n",
    "                    \"threatEntries\": [{\"url\": url}]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {api_key}\"\n",
    "            }\n",
    "\n",
    "            response = requests.post(api_url, json=payload, headers=headers)\n",
    "\n",
    "            if response.ok:\n",
    "                threat_matches = response.json().get(\"matches\", [])\n",
    "                if threat_matches:\n",
    "                    return False  # URL found in threat database, considered untrusted\n",
    "\n",
    "            return True\n",
    "        \n",
    "        def trust_check_virustotal(url, api_key):\n",
    "            api_url = 'https://www.virustotal.com/api/v3/urls'\n",
    "\n",
    "            headers = {\n",
    "                'x-apikey': api_key\n",
    "            }\n",
    "\n",
    "            params = {\n",
    "                'url': url\n",
    "            }\n",
    "\n",
    "            response = requests.get(api_url, headers=headers, params=params)\n",
    "\n",
    "            if response.ok:\n",
    "                data = response.json()\n",
    "                if data['data']['attributes']['last_analysis_stats']['malicious'] > 0:\n",
    "                    return False  # URL found to be malicious, considered untrusted\n",
    "\n",
    "            return True  # URL not found to be malicious, considered trusted\n",
    "        \n",
    "        def trust_check_urlscan(url, api_key):\n",
    "            api_url = 'https://urlscan.io/api/v1/scan/'\n",
    "\n",
    "            headers = {\n",
    "                'Content-Type': 'application/json',\n",
    "                'API-Key': api_key\n",
    "            }\n",
    "\n",
    "            data = {\n",
    "                'url': url\n",
    "            }\n",
    "\n",
    "            response = requests.post(api_url, headers=headers, json=data)\n",
    "\n",
    "            if response.ok:\n",
    "                result = response.json()\n",
    "                if result.get('message') == 'Submission successful':\n",
    "                    scan_id = result.get('uuid')\n",
    "                    report_url = f'https://urlscan.io/result/{scan_id}/'\n",
    "\n",
    "                    # Check the report for trustworthiness manually or process the response as needed\n",
    "\n",
    "                    report_url  # Return the URLScan.io report URL\n",
    "\n",
    "                if report_url!= None:\n",
    "                    print(report_url)\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            return False\n",
    "        \n",
    "        def get_certificate_age(url):\n",
    "            try:\n",
    "                hostname = self._extract_hostname(url)\n",
    "                print(hostname)\n",
    "                context = ssl.create_default_context()\n",
    "                with socket.create_connection((hostname, 443), timeout=3) as sock:\n",
    "                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
    "                        cert = ssock.getpeercert()\n",
    "                        cert_expiry = cert['notAfter']\n",
    "                        cert_creation =  cert['notBefore']\n",
    "                        creation = datetime.strptime(cert_creation, \"%b %d %H:%M:%S %Y %Z\")\n",
    "                        expiry_date = datetime.strptime(cert_expiry, \"%b %d %H:%M:%S %Y %Z\")\n",
    "                        current_date = datetime.now()\n",
    "                        age = expiry_date - creation\n",
    "                        return age.days\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Certificate Age not found returning default')\n",
    "                return 0\n",
    "            \n",
    "        http_status = using_https(self.url)\n",
    "        trust_status = trust_check_googlesafebrowsing(self.url, self.gsb_api) and trust_check_virustotal(self.url, self.vt_api) and trust_check_urlscan(self.url, self.us_api)\n",
    "        cert_age =  get_certificate_age(self.url)\n",
    "\n",
    "        if http_status and trust_status and (cert_age >= 30):\n",
    "            return 1\n",
    "        elif not http_status and trust_status and (cert_age >= 30):\n",
    "            return 1\n",
    "        elif http_status and not trust_status :\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    ## 9 - get_domain_registration_length_status\n",
    "    def get_domain_registration_length_status(self):\n",
    "        try:\n",
    "            data = self._get_whois_data()\n",
    "\n",
    "            # Extract the registration date from the API response\n",
    "            creation_date_str = data['WhoisRecord']['registryData']['createdDate']\n",
    "            creation_date = datetime.strptime(creation_date_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "            # Calculate the registration length\n",
    "            today = datetime.now()\n",
    "            registration_length = today - creation_date\n",
    "\n",
    "            if registration_length.days < 365:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_domain_registration_length_status.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "    ## 10 - get_favicon_links_status\n",
    "    def get_favicon_links_status(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "            response = requests.get(self.url, headers=headers, timeout=3.5)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            favicon_links = []\n",
    "\n",
    "            # Find all <link> tags with rel=\"icon\" or rel=\"shortcut icon\"\n",
    "            link_tags = soup.find_all('link', rel=['icon', 'shortcut icon'])\n",
    "\n",
    "            for link_tag in link_tags:\n",
    "                favicon_url = link_tag.get('href')\n",
    "\n",
    "                # Convert relative URLs to absolute URLs\n",
    "                favicon_url = urljoin(self.url, favicon_url)\n",
    "\n",
    "                favicon_links.append(favicon_url)\n",
    "\n",
    "            if len(favicon_links) > 0:\n",
    "                for link in favicon_links:\n",
    "                    if self._get_domain_external(link) == domain:\n",
    "                        return 1\n",
    "                    return -1\n",
    "            return -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_favicon_links_status.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "    ##11 - Port\n",
    "    def is_non_standard_port(self):\n",
    "        parsed_url = urlparse(self.url)\n",
    "        non_std_port_status = parsed_url.port is not None and parsed_url.port not in (80, 443)\n",
    "        if non_std_port_status:\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    ## 12 - Https token\n",
    "\n",
    "    def https_in_domain_status(self):\n",
    "        domain = self._get_domain()\n",
    "        if 'https' in domain or 'http' in domain:\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    ## 13 - Embedded objects link\n",
    "    def get_embedded_object_links_status(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            response = self._get_response(self.url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            embedded_links = []\n",
    "\n",
    "            # Find all image tags\n",
    "            for img in soup.find_all('img'):\n",
    "                img_src = img.get('src')\n",
    "                if img_src:\n",
    "                    embedded_links.append(urljoin(self.url, img_src))\n",
    "\n",
    "            # Find all video tags\n",
    "            for video in soup.find_all('video'):\n",
    "                video_src = video.get('src')\n",
    "                if video_src:\n",
    "                    embedded_links.append(urljoin(self.url, video_src))\n",
    "\n",
    "            # Find all audio tags\n",
    "            for audio in soup.find_all('audio'):\n",
    "                audio_src = audio.get('src')\n",
    "                if audio_src:\n",
    "                    embedded_links.append(urljoin(self.url, audio_src))\n",
    "\n",
    "            # Find all object tags\n",
    "            for obj in soup.find_all('object'):\n",
    "                obj_data = obj.get('data')\n",
    "                if obj_data:\n",
    "                    embedded_links.append(urljoin(self.url, obj_data))\n",
    "\n",
    "            print(f'Embedded obj parsed links - {embedded_links}')\n",
    "        \n",
    "            similarity_perc = self._get_dissimilarity(embedded_links, domain)\n",
    "\n",
    "            if similarity_perc <= 22:\n",
    "                return 1\n",
    "            elif similarity_perc > 22 and similarity_perc <= 61:\n",
    "                return 0\n",
    "            else: \n",
    "                return -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_embedded_object_links_status.__name__} failed returning default\" )\n",
    "            return 0\n",
    "        \n",
    "    ## 14 - Get Anchor link status\n",
    "    def get_anchor_links_status(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            response = self._get_response(self.url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            anchor_links = []\n",
    "\n",
    "            # Find all anchor tags\n",
    "            for anchor in soup.find_all('a'):\n",
    "                href = anchor.get('href')\n",
    "                if href:\n",
    "                    anchor_links.append(urljoin(self.url, href))\n",
    "\n",
    "            print(f'anchor parsed links - {anchor_links}')\n",
    "        \n",
    "            similarity_perc = self._get_dissimilarity(anchor_links, domain)\n",
    "\n",
    "            if similarity_perc < 36:\n",
    "                return 1\n",
    "            elif similarity_perc >= 36 and similarity_perc <= 71:\n",
    "                return 0\n",
    "            else: \n",
    "                return -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_anchor_links_status.__name__} failed returning default\" )\n",
    "            return 0\n",
    "        \n",
    "    ## 15 - Links from tags\n",
    "    def get_links_from_tags(self):\n",
    "\n",
    "        domain = self._get_domain()\n",
    "\n",
    "        try:\n",
    "            response = self._get_response(self.url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            links = []\n",
    "\n",
    "            # Find links from meta tags\n",
    "            for meta in soup.find_all('meta'):\n",
    "                meta_content = meta.get('content')\n",
    "                if meta_content:\n",
    "                    links.append(meta_content)\n",
    "\n",
    "            # Find links from link tags\n",
    "            for link in soup.find_all('link'):\n",
    "                link_href = link.get('href')\n",
    "                if link_href:\n",
    "                    links.append(urljoin(self.url, link_href))\n",
    "\n",
    "            # Find links from script tags\n",
    "            for script in soup.find_all('script'):\n",
    "                script_src = script.get('src')\n",
    "                if script_src:\n",
    "                    links.append(urljoin(self.url, script_src))\n",
    "\n",
    "            ## The metadata tags may contain strings with the url, parsing the url form the link text recieved from the url\n",
    "            parsed_links = self._extract_links_from_strings(links)\n",
    "            print(f'tags parsed links - {parsed_links}')\n",
    "\n",
    "            similarity_perc =  self._get_dissimilarity(parsed_links, domain)\n",
    "\n",
    "            if similarity_perc < 46:\n",
    "                return 1\n",
    "            elif similarity_perc >= 46 and similarity_perc <= 71:\n",
    "                return 0\n",
    "            else: \n",
    "                return -1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_links_from_tags.__name__} failed returning default\" )\n",
    "            return 0\n",
    "        \n",
    "    ## 16 - Server form Handeler\n",
    "    def check_form_sfh(self):\n",
    "        try:\n",
    "            # Parse the URL to extract the domain\n",
    "            parsed_url = urlparse(self.url)\n",
    "            domain = self._get_domain()\n",
    "\n",
    "            # Check if the URL is a form\n",
    "            if self._check_form(self.url):\n",
    "                # Check if the SFH is about:blank or empty\n",
    "                if 'sfh' in parsed_url.query.lower() and ('about:blank' in parsed_url.query.lower() or parsed_url.query.lower() == 'sfh='):\n",
    "                    return -1\n",
    "                # Check if the SFH refers to a different domain\n",
    "                elif 'sfh' in parsed_url.query.lower() and parsed_url.query.lower().startswith('sfh=') and domain.lower() != parsed_url.query[4:].lower():\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                print(\"Given url is not a form\")\n",
    "                return 1  # Not a form\n",
    "            \n",
    "        except:\n",
    "            print(f\"Function{self.check_form_sfh.__name__} failed returning default\" )\n",
    "            return 1\n",
    "        \n",
    "    ## 17 - Submitting to mail\n",
    "    def check_form_redirection_to_mail(self):\n",
    "        try:\n",
    "            response = self._get_response(self.url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            if self._check_form(self.url):\n",
    "                form = soup.find('form')\n",
    "                if form:\n",
    "                    # Check for \"mailto:\" in the form action attribute\n",
    "                    action = form.get('action', '').lower()\n",
    "                    if action.startswith('mailto:'):\n",
    "                        return -1\n",
    "\n",
    "                    # Check for JavaScript-based redirection\n",
    "                    script_tags = soup.find_all('script')\n",
    "                    for script in script_tags:\n",
    "                        if 'location.href' in script.text or 'window.open' in script.text:\n",
    "                            return -1\n",
    "\n",
    "                    # Check for server-side redirection\n",
    "                    # Inspect the server-side code associated with the form submission\n",
    "\n",
    "                    # Check for PHP mail() function\n",
    "                    php_tags = soup.find_all('php')\n",
    "                    for php in php_tags:\n",
    "                        if 'mail(' in php.text:\n",
    "                            return -1\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Function{self.check_form_redirection_to_mail.__name__} failed returning default\" )\n",
    "            return 1\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    ## 18 - Abnormal Url\n",
    "    def is_abnormal_url(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            \n",
    "            data = self._get_whois_data()\n",
    "\n",
    "            if 'ErrorMessage' in data:\n",
    "                # Error occurred while making the API request\n",
    "                print(f\"Error: {data['ErrorMessage']}\")\n",
    "                return False\n",
    "\n",
    "            if 'WhoisRecord' in data:\n",
    "                whois_record = data['WhoisRecord']\n",
    "                if whois_record['domainName'] == domain:\n",
    "                    return 1\n",
    "\n",
    "            return -1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.is_abnormal_url.__name__} failed returning default\" )\n",
    "            return -1\n",
    "    \n",
    "    ## 19 - Redirects\n",
    "    def get_redirects_status(self):\n",
    "        try:\n",
    "            parsed_url = urlparse(self.url)\n",
    "        \n",
    "            if not parsed_url.scheme:\n",
    "                url = \"https://\" + url\n",
    "            else:\n",
    "                url = self.url\n",
    "            \n",
    "            response = requests.get(url, allow_redirects=True, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                num_redirects = len(response.history)\n",
    "                \n",
    "                if num_redirects <= 1:\n",
    "                    return 1\n",
    "                elif num_redirects >= 2 and num_redirects <4:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return -1\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.get_redirects_status.__name__} failed returning default\" )\n",
    "            return 0\n",
    "        \n",
    "    ## 20 - OnMouseOver\n",
    "    def check_onmouseover_status_bar(self):\n",
    "        try:    \n",
    "            response = self._get_response(self.url)\n",
    "            source_code = response.text\n",
    "            \n",
    "            if 'onMouseOver' in source_code:\n",
    "                if 'window.status' in source_code or 'window.defaultStatus' in source_code:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "        except:\n",
    "            print(f\"Function{self.check_onmouseover_status_bar.__name__} failed returning default\" )\n",
    "            return 1\n",
    "        \n",
    "    ## 21 - Right Click Disabled\n",
    "\n",
    "    def check_right_click_disabled(self):\n",
    "        try:  \n",
    "            response = self._get_response(self.url)\n",
    "            source_code = response.text\n",
    "            if re.findall(r\"event.button ?== ?2\", source_code):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "             print(f\"Function{self.check_right_click_disabled.__name__} failed returning default\" )\n",
    "             return 1\n",
    "        \n",
    "    ## 22 - Popup Window\n",
    "    def check_popup_with_text_fields(self):\n",
    "        try:\n",
    "            response = self._get_response(self.url)\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Find all <script> tags in the HTML\n",
    "            script_tags = soup.find_all('script')\n",
    "\n",
    "            for script_tag in script_tags:\n",
    "                if 'window.open' in str(script_tag) or 'form-popup' in str(script_tag) or 'alert(' in str(script_tag):\n",
    "                    # Check if the popup window has text fields\n",
    "                    if 'input type=\"text\"' in str(script_tag):\n",
    "                        return -1  # Popup window with text fields found\n",
    "\n",
    "            return 1  # No popup windows with text fields found\n",
    "        except:\n",
    "            print(f\"Function{self.check_popup_with_text_fields.__name__} failed returning default\" )\n",
    "            return 1\n",
    "        \n",
    "    ## 23 - Using IFrame\n",
    "    def using_iframe(self):\n",
    "        try:\n",
    "            response = self._get_response(self.url)\n",
    "\n",
    "            if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "             print(f\"Function{self.using_iframe.__name__} failed returning default\" )\n",
    "             return 1\n",
    "        \n",
    "    ## 24 - Age of domian\n",
    "    def domain_age(self):\n",
    "        try:\n",
    "            data =self._get_whois_data()\n",
    "\n",
    "            if 'ErrorMessage' in data:\n",
    "                # Error occurred while making the API request\n",
    "                print(f\"Error: {data['ErrorMessage']}\")\n",
    "                return -1\n",
    "\n",
    "            if 'WhoisRecord' in data:\n",
    "                whois_record = data['WhoisRecord']\n",
    "                if whois_record['estimatedDomainAge'] <= 183:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return 1\n",
    "\n",
    "            return -1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.domain_age.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "    ## 25 - DNS Record\n",
    "    def check_dns_record(self):\n",
    "        try:\n",
    "            data =self._get_whois_data()\n",
    "\n",
    "            if 'ErrorMessage' in data:\n",
    "                # Error occurred while making the API request\n",
    "                print(f\"Error: {data['ErrorMessage']}\")\n",
    "                return -1\n",
    "\n",
    "            if 'WhoisRecord' in data:\n",
    "                whois = data['WhoisRecord']\n",
    "\n",
    "                if \"domainName\" in whois and len(whois['domainName'])> 0 and whois['domainName'] == self._get_domain():\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            return -1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Function{self.check_dns_record.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "    ## 26 - Page Rank\n",
    "\n",
    "    def get_page_rank_status(self):\n",
    "        try:\n",
    "            domain = self._get_domain()\n",
    "            url = f\"https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D={domain}\"\n",
    "            headers = {\n",
    "                \"API-OPR\": self.opr_api\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, headers=headers, timeout = 3.5)\n",
    "            data = response.json()\n",
    "            page_rank = data['response'][0]['page_rank_decimal']\n",
    "            if page_rank >2:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "        except:\n",
    "            print(f\"Function{self.get_page_rank_status.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "    ## 27 - Google Index\n",
    "    def check_google_indexed(self):\n",
    "        try:    \n",
    "            google = \"https://www.google.com/search?q=site:\" + self.url + \"&hl=en\"\n",
    "            response = requests.get(google, cookies={\"CONSENT\": \"YES+1\"})\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            not_indexed = re.compile(\"did not match any documents\")\n",
    "\n",
    "            if soup(text=not_indexed):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            print(f\"Function{self.check_google_indexed.__name__} failed returning default\" )\n",
    "            return -1\n",
    "        \n",
    "\n",
    "    def get_feature_list(self):\n",
    "        feature_list = []\n",
    "        \n",
    "        feature_list.append(self.having_IP_Address())\n",
    "        feature_list.append(self.URL_Length())\n",
    "        feature_list.append(self.Shortining_Service())\n",
    "        feature_list.append(self.having_At_Symbol())\n",
    "        feature_list.append(self.double_slash_redirecting())\n",
    "        feature_list.append(self.Prefix_Suffix())\n",
    "        feature_list.append(self.having_Sub_Domain())\n",
    "        feature_list.append(self.SSLfinal_State())\n",
    "        feature_list.append(self.get_domain_registration_length_status())\n",
    "        feature_list.append(self.get_favicon_links_status())\n",
    "        feature_list.append(self.is_non_standard_port())\n",
    "        feature_list.append(self.https_in_domain_status())\n",
    "        feature_list.append(self.get_embedded_object_links_status())\n",
    "        feature_list.append(self.get_anchor_links_status())\n",
    "        feature_list.append(self.get_links_from_tags())\n",
    "        feature_list.append(self.check_form_sfh())\n",
    "        feature_list.append(self.check_form_redirection_to_mail())\n",
    "        feature_list.append(self.is_abnormal_url())\n",
    "        feature_list.append(self.get_redirects_status())\n",
    "        feature_list.append(self.check_onmouseover_status_bar())\n",
    "        feature_list.append(self.check_right_click_disabled())\n",
    "        feature_list.append(self.check_popup_with_text_fields())\n",
    "        feature_list.append(self.using_iframe())\n",
    "        feature_list.append(self.domain_age())\n",
    "        feature_list.append(self.check_dns_record())\n",
    "        feature_list.append(self.get_page_rank_status())\n",
    "        feature_list.append(self.check_google_indexed())\n",
    "\n",
    "        return np.array(feature_list).reshape(1,27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn.org\n",
      "https://urlscan.io/result/11550c33-9e93-4dcf-97da-e49c95c85455/\n",
      "scikit-learn.org\n",
      "Embedded obj parsed links - ['https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_release_highlights_1_1_0_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_release_highlights_0_23_0_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_stack_predictors_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_permutation_importance_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_pipeline_display_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_estimator_representation_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_set_output_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_missing_values_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_iterative_imputer_variants_comparison_thumb.png', 'https://scikit-learn.org/stable/_images/sphx_glr_plot_column_transformer_mixed_types_thumb.png']\n",
      "anchor parsed links - ['https://scikit-learn.org/stable/index.html', 'https://scikit-learn.org/stable/install.html', 'https://scikit-learn.org/stable/user_guide.html', 'https://scikit-learn.org/stable/modules/classes.html', 'https://scikit-learn.org/stable/auto_examples/index.html', 'https://blog.scikit-learn.org/', 'https://scikit-learn.org/stable/getting_started.html', 'https://scikit-learn.org/stable/tutorial/index.html', 'https://scikit-learn.org/stable/whats_new/v1.2.html', 'https://scikit-learn.org/stable/glossary.html', 'https://scikit-learn.org/dev/developers/index.html', 'https://scikit-learn.org/stable/faq.html', 'https://scikit-learn.org/stable/support.html', 'https://scikit-learn.org/stable/related_projects.html', 'https://scikit-learn.org/stable/roadmap.html', 'https://scikit-learn.org/stable/governance.html', 'https://scikit-learn.org/stable/about.html', 'https://github.com/scikit-learn/scikit-learn', 'https://scikit-learn.org/dev/versions.html', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html', 'https://scikit-learn.org/stable/getting_started.html', 'https://scikit-learn.org/stable/tutorial/index.html', 'https://scikit-learn.org/stable/whats_new/v1.2.html', 'https://scikit-learn.org/stable/glossary.html', 'https://scikit-learn.org/dev/developers/index.html', 'https://scikit-learn.org/stable/faq.html', 'https://scikit-learn.org/stable/support.html', 'https://scikit-learn.org/stable/related_projects.html', 'https://scikit-learn.org/stable/roadmap.html', 'https://scikit-learn.org/stable/governance.html', 'https://scikit-learn.org/stable/about.html', 'https://github.com/scikit-learn/scikit-learn', 'https://scikit-learn.org/dev/versions.html', 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html', 'https://scikit-learn.org/stable/modules/classes.html', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html', 'http://scikit-learn.org/dev/versions.html', 'https://scikit-learn.org/stable/about.html#citing-scikit-learn', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit_transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_feature_names_out', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_params', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.inverse_transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_output', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_params', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#examples-using-sklearn-impute-simpleimputer', 'https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn-impute-simpleimputer', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/impute/_base.py#L142', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer', 'https://scikit-learn.org/stable/modules/impute.html#impute', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator', 'https://scikit-learn.org/stable/glossary.html#term-fit', 'https://scikit-learn.org/stable/glossary.html#term-fit', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit_transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_feature_names_out', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_params', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.inverse_transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_output', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_params', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.transform', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/impute/_base.py#L363', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/base.py#L850', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.fit_transform', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/impute/_base.py#L692', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_feature_names_out', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/base.py#L153', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.get_params', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/impute/_base.py#L623', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.inverse_transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/utils/_set_output.py#L208', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_output', 'https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/base.py#L177', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.set_params', 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline', 'https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/impute/_base.py#L535', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer.transform', 'https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#examples-using-sklearn-impute-simpleimputer', 'https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_1_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-1-0-py', 'https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py', 'https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py', 'https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py', 'https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_pipeline_display.html#sphx-glr-auto-examples-miscellaneous-plot-pipeline-display-py', 'https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_estimator_representation.html#sphx-glr-auto-examples-miscellaneous-plot-estimator-representation-py', 'https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py', 'https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py', 'https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py', 'https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py', 'https://scikit-learn.org/stable/_sources/modules/generated/sklearn.impute.SimpleImputer.rst.txt']\n",
      "tags parsed links - ['https://docutils.sourceforge.io', 'http://sklearn.impute.Simple', 'https://scikit-learn', 'http://sklearn.impute.Simple', 'https://scikit-learn', 'http://sklearn.impute.Simple', 'http://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://scikit-learn.org', 'https://www.google-analytics.com', 'https://views.scientific-python.org', 'https://cdn.jsdelivr.net', 'https://scikit-learn.org']\n"
     ]
    }
   ],
   "source": [
    "features1 = FeatureExtractor(url = \"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\").get_feature_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow.com\n",
      "https://urlscan.io/result/9877cf55-f6bc-4d74-bb3a-2e759fcf7305/\n",
      "stackoverflow.com\n",
      "Embedded obj parsed links - ['https://cdn.sstatic.net/Img/teams/teams-illo-free-sidebar-promo.svg?v=47faa659a05e', 'https://www.gravatar.com/avatar/327dc718c142e3ea70e5db4b38050a36?s=64&d=identicon&r=PG', 'https://i.stack.imgur.com/LrLr2.jpg?s=64&g=1', 'https://i.stack.imgur.com/Bmn9s.jpg?s=64&g=1', 'https://stackoverflow.com/posts/61388023/ivc/357d?prg=9624928f-6184-4eb5-bd3a-c5f248983a42']\n",
      "anchor parsed links - ['https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com', 'https://stackoverflow.co/', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/teams', 'https://stackoverflow.com/questions', 'https://stackoverflow.com/teams', 'https://stackoverflow.co/talent', 'https://stackoverflow.co/advertising', 'https://stackoverflow.co/', 'https://stackoverflow.com', 'https://stackoverflow.com', 'https://stackoverflow.com/help', 'https://chat.stackoverflow.com/?tab=site&host=stackoverflow.com', 'https://meta.stackoverflow.com', 'https://stackoverflow.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f61388023%2fhow-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f61388023%2fhow-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackexchange.com/sites', 'https://stackoverflow.blog', 'https://stackoverflow.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f61388023%2fhow-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f61388023%2fhow-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/', 'https://stackoverflow.com/questions', 'https://stackoverflow.com/tags', 'https://stackoverflow.com/users', 'https://stackoverflow.com/jobs/companies?so_medium=stackoverflow&so_source=SiteNav', 'javascript:void(0)', 'https://stackoverflow.com/collectives', 'https://try.stackoverflow.co/why-teams/?utm_source=so-owned&utm_medium=side-bar&utm_campaign=campaign-38&utm_content=cta', 'https://stackoverflow.co/teams', 'javascript:void(0)', 'https://stackoverflowteams.com/teams/create/free?utm_source=so-owned&utm_medium=side-bar&utm_campaign=campaign-38&utm_content=cta', 'https://stackoverflow.com/collectives', 'https://stackoverflow.co/teams', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/questions/ask', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally?lastactivity', 'https://stackoverflow.com/posts/61388023/timeline', 'https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html', 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba', 'https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html', 'https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally', 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba', 'https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier', 'https://stackoverflow.com/questions/tagged/python', 'https://stackoverflow.com/questions/tagged/machine-learning', 'https://stackoverflow.com/questions/tagged/scikit-learn', 'https://stackoverflow.com/questions/tagged/probability', 'https://stackoverflow.com/questions/tagged/mlp', 'https://stackoverflow.com/q/61388023', 'https://stackoverflow.com/posts/61388023/edit', 'https://stackoverflow.com/posts/61388023/revisions', 'https://stackoverflow.com/users/4685471/desertnaut', 'https://stackoverflow.com/users/4685471/desertnaut', 'https://stackoverflow.com/users/4876561/artemis', 'https://stackoverflow.com/users/4876561/artemis', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally?answertab=scoredesc#tab-top', 'https://stackoverflow.com/posts/61388395/timeline', 'https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/neural_network/_multilayer_perceptron.py', 'https://stackoverflow.com/a/61388395', 'https://stackoverflow.com/posts/61388395/edit', 'https://stackoverflow.com/users/6837288/giuseppe-angora', 'https://stackoverflow.com/users/6837288/giuseppe-angora', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/help/how-to-answer', 'https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f61388023%2fhow-does-sklearns-mlp-predict-proba-function-work-internally%23new-answer', 'https://stackoverflow.com/legal/terms-of-service/public', 'https://stackoverflow.com/legal/privacy-policy', 'https://stackoverflow.com/legal/cookie-policy', 'https://stackoverflow.com/questions/tagged/python', 'https://stackoverflow.com/questions/tagged/machine-learning', 'https://stackoverflow.com/questions/tagged/scikit-learn', 'https://stackoverflow.com/questions/tagged/probability', 'https://stackoverflow.com/questions/tagged/mlp', 'https://stackoverflow.com/questions/ask', 'https://stackoverflow.blog/2023/05/17/keep-em-separated-get-better-maintainability-in-web-projects-using-the-model-view-controller-pattern/', 'https://stackoverflow.blog/2023/05/19/building-zero-tier-systems-on-bare-metal-ep-572/', 'https://meta.stackexchange.com/questions/388860/we-are-updating-our-code-of-conduct-and-we-would-like-your-feedback', 'https://meta.stackexchange.com/questions/389262/ai-ml-tool-examples-part-3-title-drafting-assistant', 'https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned', 'https://meta.stackoverflow.com/questions/418968/the-connect-tag-is-being-burninated', 'https://meta.stackoverflow.com/questions/424638/stack-overflow-will-be-testing-a-title-drafting-assistant-and-we-d-like-your-in', 'https://meta.stackoverflow.com/questions/424716/we-are-graduating-the-related-questions-using-machine-learning-experiment', 'https://stackoverflow.com/q/15111408', 'https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally?noredirect=1', 'https://stackoverflow.com/q/63490533', 'https://stackoverflow.com/questions/63490533/how-does-the-predict-proba-function-in-lightgbm-work-internally?noredirect=1', 'https://stackoverflow.com/q/739654', 'https://stackoverflow.com/questions/739654/how-do-i-make-function-decorators-and-chain-them-together', 'https://stackoverflow.com/q/3277367', 'https://stackoverflow.com/questions/3277367/how-does-pythons-super-work-with-multiple-inheritance', 'https://stackoverflow.com/q/5900578', 'https://stackoverflow.com/questions/5900578/collections-defaultdict-difference-with-normal-dict', 'https://stackoverflow.com/q/17330160', 'https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python', 'https://stackoverflow.com/q/26478000', 'https://stackoverflow.com/questions/26478000/converting-linearsvcs-decision-function-to-probabilities-scikit-learn-python', 'https://stackoverflow.com/q/30972029', 'https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work', 'https://stackoverflow.com/q/42241991', 'https://stackoverflow.com/questions/42241991/why-does-predict-proba-function-print-the-probabilities-in-reverse-order', 'https://stackoverflow.com/q/44345219', 'https://stackoverflow.com/questions/44345219/how-do-i-correctly-manually-recreate-sklearn-python-logistic-regression-predic', 'https://stackoverflow.com/q/63490533', 'https://stackoverflow.com/questions/63490533/how-does-the-predict-proba-function-in-lightgbm-work-internally', 'https://stackoverflow.com/q/64590557', 'https://stackoverflow.com/questions/64590557/how-to-get-the-predict-proba-for-the-class-predicted-by-predict-in-random-fo', 'https://stackexchange.com/questions?tab=hot', 'https://english.stackexchange.com/questions/607760/is-there-a-name-for-such-political-abbreviations-as-libfem-for-liberal-feminis', 'https://unix.stackexchange.com/questions/746345/why-does-ddrescue-not-use-distinct-mapfiles-for-read-and-write-errors-and-how', 'https://boardgames.stackexchange.com/questions/58800/what-does-a-double-mean-when-made-by-a-partner-who-has-already-passed', 'https://physics.stackexchange.com/questions/764460/why-is-my-dryer-radioactive', 'https://matheducators.stackexchange.com/questions/26437/adjusting-a-bonus-points-system-to-more-equitably-benefit-struggling-students', 'https://law.stackexchange.com/questions/92510/liability-of-products-that-are-unsafe-by-design-like-skateboards', 'https://music.stackexchange.com/questions/129957/chromatic-modes', 'https://english.stackexchange.com/questions/607701/word-for-the-loss-of-one-parent', 'https://cs.stackexchange.com/questions/160234/why-is-backpatching-needed-during-intermediate-code-generation-for-what-purpose', 'https://tex.stackexchange.com/questions/686146/commutative-diagrams-side-by-side', 'https://travel.stackexchange.com/questions/181253/doing-laundry-in-european-hostels', 'https://bicycles.stackexchange.com/questions/88847/what-year-is-this-mongoose', 'https://mathoverflow.net/questions/447114/why-does-the-2-category-of-groups-have-some-strict-coinserters-but-not-stric', 'https://earthscience.stackexchange.com/questions/25201/how-is-the-atmosphere-expanding-in-all-directions-if-gravity-is-holding-the-atmo', 'https://worldbuilding.stackexchange.com/questions/246298/why-cant-autonomous-delivery-drones-solve-world-hunger', 'https://tex.stackexchange.com/questions/686130/customize-tikz-circle-labels-to-flow-along-with-the-circle-curve', 'https://codegolf.stackexchange.com/questions/261077/turn-strings-into-hexagonal-spirals', 'https://cooking.stackexchange.com/questions/124207/what-can-i-cook-in-kamado-joe-egg-that-i-cant-cook-in-weber-master-touch-grill', 'https://math.stackexchange.com/questions/4702778/find-the-value-of-cm-on-the-circle-below', 'https://puzzling.stackexchange.com/questions/120890/any-fans-of-the-big-bang-theory', 'https://aviation.stackexchange.com/questions/99153/at-non-controlled-fields-what-are-the-regulations-concerning-line-up-and-wait', 'https://politics.stackexchange.com/questions/79634/is-the-leftist-tendency-in-academia-restricted-to-humanities', 'https://law.stackexchange.com/questions/92535/can-artists-file-for-plagiarism-if-their-art-is-used-in-ai-models-to-make-ai-art', 'https://diy.stackexchange.com/questions/272667/do-one-gang-power-outlets-with-more-than-two-jacks-exist', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/feeds/question/61388023', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com', 'https://stackoverflow.com', 'https://stackoverflow.com/questions', 'https://stackoverflow.com/help', 'https://stackoverflow.co/', 'https://stackoverflow.co/teams', 'https://stackoverflow.co/advertising', 'https://stackoverflow.co/collectives', 'https://stackoverflow.co/talent', 'https://stackoverflow.co/', 'https://stackoverflow.co/', 'https://stackoverflow.co/company/press', 'https://stackoverflow.co/company/work-here', 'https://stackoverflow.com/legal', 'https://stackoverflow.com/legal/privacy-policy', 'https://stackoverflow.com/legal/terms-of-service', 'https://stackoverflow.co/company/contact', 'https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally', 'https://stackoverflow.com/legal/cookie-policy', 'https://stackexchange.com', 'https://stackexchange.com/sites#technology', 'https://stackexchange.com/sites#culturerecreation', 'https://stackexchange.com/sites#lifearts', 'https://stackexchange.com/sites#science', 'https://stackexchange.com/sites#professional', 'https://stackexchange.com/sites#business', 'https://api.stackexchange.com/', 'https://data.stackexchange.com/', 'https://stackoverflow.blog?blb=1', 'https://www.facebook.com/officialstackoverflow/', 'https://twitter.com/stackoverflow', 'https://linkedin.com/company/stack-overflow', 'https://www.instagram.com/thestackoverflow', 'https://stackoverflow.com/help/licensing', 'https://stackoverflow.com/legal/cookie-policy']\n",
      "tags parsed links - ['https://stackoverflow.com', 'https://cdn.sstatic.net', 'http://stackoverflow.com', 'https://cdn.sstatic.net', 'https://cdn.sstatic.net', 'https://cdn.sstatic.net', 'https://stackoverflow.com', 'https://stackoverflow.com', 'https://cdn.sstatic.net', 'https://cdn.sstatic.net', 'https://stackoverflow.com', 'https://cdn.sstatic.net', 'https://cdn.sstatic.net', 'https://ajax.googleapis.com', 'https://cdn.sstatic.net', 'https://cdn.sstatic.net', 'https://www.googletagmanager.com']\n"
     ]
    }
   ],
   "source": [
    "features2 = FeatureExtractor(url = \"https://stackoverflow.com/questions/61388023/how-does-sklearns-mlp-predict-proba-function-work-internally\").get_feature_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../artifacts/model_training/saved_models/model.joblib', 'rb') as model_file:\n",
    "    mlp_model = joblib.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DS_Projects\\Phishing-Domain-Detection\\phish_predictor_env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.predict(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DS_Projects\\Phishing-Domain-Detection\\phish_predictor_env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.predict(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DS_Projects\\Phishing-Domain-Detection\\phish_predictor_env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999891"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.predict_proba(features1)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DS_Projects\\Phishing-Domain-Detection\\phish_predictor_env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.predict_proba(features2)[0,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
